Arguments are...
log_dir: ./qm9_trained_models/GeoMol100_logP_64_combine_global
data_dir: data/QM9/qm9/
split_path: data/QM9/splits/split0.npy
trained_local_model: None
restart_dir: None
dataset: qm9_deepergcn
seed: 0
n_epochs: 250
warmup_epochs: 2
batch_size: 16
lr: 0.001
num_workers: 4
optimizer: adam
scheduler: plateau
verbose: False
model_dim: 100
random_vec_dim: 10
random_vec_std: 1
random_alpha: False
n_true_confs: 10
n_model_confs: 10
gnn1_depth: 3
gnn1_n_layers: 2
gnn2_depth: 3
gnn2_n_layers: 2
encoder_n_head: 2
coord_pred_n_layers: 2
d_mlp_n_layers: 1
h_mol_mlp_n_layers: 1
alpha_mlp_n_layers: 2
c_mlp_n_layers: 1
global_transformer: False
loss_type: ot_emd
teacher_force: False
separate_opts: False
no_h_mol: False
MolR_emb: False
MolR_node_emb: False
embed_path: MolR/saved/sage_50
embeddings_dim: 64
combine_global: True
utilize_fingerprints: False
augment_fingerprints: False
utilize_deepergcn: True
dg_molecular_property: logP

Model parameters are:
hyperparams:
  model_dim: 100
  random_vec_dim: 10
  random_vec_std: 1
  global_transformer: False
  n_true_confs: 10
  n_model_confs: 10
  gnn1:
    depth: 3
    n_layers: 2
  gnn2:
    depth: 3
    n_layers: 2
  encoder:
    n_head: 2
  coord_pred:
    n_layers: 2
  d_mlp:
    n_layers: 1
  h_mol_mlp:
    n_layers: 1
  alpha_mlp:
    n_layers: 2
  c_mlp:
    n_layers: 1
  loss_type: ot_emd
  teacher_force: False
  random_alpha: False
  no_h_mol: False
  MolR_emb: False
  MolR_node_emb: False
  embed_path: MolR/saved/sage_50
  embeddings_dim: 64
  combine_global: True
  utilize_fingerprints: False
  augment_fingerprints: False
  utilize_deepergcn: True
  dg_molecular_property: logP
num_node_features: 74
num_edge_features: 4


Starting training...
Epoch 1: Training Loss -0.7871884475961327
Epoch 1: Validation Loss -0.46436578271880985
Epoch 2: Training Loss -1.3091685127615929
Epoch 2: Validation Loss -1.3993045602525984
Epoch 3: Training Loss -1.513836821746826
Epoch 3: Validation Loss -1.5210322860687497
Epoch 4: Training Loss -1.5503493328094482
Epoch 4: Validation Loss -1.5271211578732444
Epoch 5: Training Loss -1.5672769590377809
Epoch 5: Validation Loss -1.5276509901833912
Epoch 6: Training Loss -1.605117494392395
Epoch 6: Validation Loss -1.598009190862141
Epoch 7: Training Loss -1.6074848735809326
Epoch 7: Validation Loss -1.6253257554674905
Epoch 8: Training Loss -1.6181936216354371
Epoch 8: Validation Loss -1.5981079264292641
Epoch 9: Training Loss -1.620829797744751
Epoch 9: Validation Loss -1.671301455724807
Epoch 10: Training Loss -1.638998750114441
Epoch 10: Validation Loss -1.592149267120967
Epoch 11: Training Loss -1.6446470287322998
Epoch 11: Validation Loss -1.6522081428103976
Epoch 12: Training Loss -1.6573293140411376
Epoch 12: Validation Loss -1.6556736079473344
Epoch 13: Training Loss -1.6544002252578736
Epoch 13: Validation Loss -1.6459154344740368
Epoch 14: Training Loss -1.637730354309082
Epoch 14: Validation Loss -1.6143772431782313
Epoch 15: Training Loss -0.9206173154830932
Epoch 15: Validation Loss -0.8968470503413488
Epoch 16: Training Loss -0.8880466897964477
Epoch 16: Validation Loss -0.8761394194194249
Epoch 17: Training Loss -0.8988928735733033
Epoch 17: Validation Loss -0.9097974735593038
Epoch 18: Training Loss -0.9074493197441101
Epoch 18: Validation Loss -0.9146000373931158
Epoch 19: Training Loss -0.9127686893463135
Epoch 19: Validation Loss -0.9140720036294725
Epoch 20: Training Loss -0.9097916873931885
Epoch 20: Validation Loss -0.9074272702610682
Epoch 21: Training Loss -0.9164385442733765
Epoch 21: Validation Loss -0.9279939333597819
Epoch 22: Training Loss -0.9190442957878113
Epoch 22: Validation Loss -0.9394754937716893
Epoch 23: Training Loss -0.9219117133140564
Epoch 23: Validation Loss -0.9829799977559892
Epoch 24: Training Loss -0.9226794186592102
Epoch 24: Validation Loss -0.9154696814597599
Epoch 25: Training Loss -0.9032801781177521
Epoch 25: Validation Loss -0.9248569058993507
Epoch 26: Training Loss -0.9180129558563233
Epoch 26: Validation Loss -0.9394820815040952
Epoch 27: Training Loss -0.9401993521690368
Epoch 27: Validation Loss -0.9341899516090514
Epoch 28: Training Loss -0.9443898432731629
Epoch 28: Validation Loss -0.9361174078214736
Epoch 29: Training Loss -0.9377535247802734
Epoch 29: Validation Loss -0.9495400985081991
Epoch 30: Training Loss -0.928401410484314
Epoch 30: Validation Loss -0.9410424478470333
Epoch 31: Training Loss -0.8923329909324647
Epoch 31: Validation Loss -0.8252013136470129
Epoch 32: Training Loss -0.8502241514205933
Epoch 32: Validation Loss -0.8604356930369422
Epoch 33: Training Loss -0.8615605373859405
Epoch 33: Validation Loss -0.8745519575618562
Epoch 34: Training Loss -0.9794795789718628
Epoch 34: Validation Loss -1.3475120370350187
Epoch 35: Training Loss -1.3854608981132508
Epoch 35: Validation Loss -1.4814205093989297
Epoch 36: Training Loss -1.5135928241729737
Epoch 36: Validation Loss -1.559358583556281
Epoch 37: Training Loss -1.5777879474639893
Epoch 37: Validation Loss -1.505347282167465
Epoch 38: Training Loss -1.6114141096115113
Epoch 38: Validation Loss -1.651807709345742
Epoch 39: Training Loss -1.6434178508758546
Epoch 39: Validation Loss -1.680470551763262
Epoch 40: Training Loss -1.669630888748169
Epoch 40: Validation Loss -1.679541614320543
Epoch 41: Training Loss -1.6832578365325928
Epoch 41: Validation Loss -1.6897842127179343
Epoch 42: Training Loss -1.6785154973983765
Epoch 42: Validation Loss -1.6900415212389022
Epoch 43: Training Loss -1.6934336744308471
Epoch 43: Validation Loss -1.7077776496372525
Epoch 44: Training Loss -1.699516978263855
Epoch 44: Validation Loss -1.6945323584571717
Epoch 45: Training Loss -1.7051929254531861
Epoch 45: Validation Loss -1.7102220966702415
Epoch 46: Training Loss -1.7079978624343872
Epoch 46: Validation Loss -1.708690883621337
Epoch 47: Training Loss -1.711427455329895
Epoch 47: Validation Loss -1.7034351314817155
Epoch 48: Training Loss -1.7076348119735718
Epoch 48: Validation Loss -1.70189482825143
Epoch 49: Training Loss -1.701540930366516
Epoch 49: Validation Loss -1.7108477732491871
Epoch 50: Training Loss -1.7120467506408692
Epoch 50: Validation Loss -1.6922007155796839
Epoch 51: Training Loss -1.712669913482666
Epoch 51: Validation Loss -1.7106474713673667
Epoch 52: Training Loss -1.7129660871505736
Epoch 52: Validation Loss -1.7047960114857508
Epoch 53: Training Loss -1.711813662147522
Epoch 53: Validation Loss -1.7236138279475863
Epoch 54: Training Loss -1.7150933107376098
Epoch 54: Validation Loss -1.7177637531643821
Epoch 55: Training Loss -1.7234540933609008
Epoch 55: Validation Loss -1.7089178581086417
Epoch 56: Training Loss -1.7201528600692748
Epoch 56: Validation Loss -1.6999478037395175
Epoch 57: Training Loss -1.718492725944519
Epoch 57: Validation Loss -1.72936417186071
Epoch 58: Training Loss -1.7216816457748414
Epoch 58: Validation Loss -1.7191934812636602
Epoch 59: Training Loss -1.722006900024414
Epoch 59: Validation Loss -1.737330200180175
Epoch 60: Training Loss -1.7288436264038085
Epoch 60: Validation Loss -1.7238433625962999
Epoch 61: Training Loss -1.7256376358032226
Epoch 61: Validation Loss -1.7333115396045504
Epoch 62: Training Loss -1.7252149507522583
Epoch 62: Validation Loss -1.7262735480353946
Epoch 63: Training Loss -1.7285310647964478
Epoch 63: Validation Loss -1.7280055246655903
Epoch 64: Training Loss -1.7307115785598755
Epoch 64: Validation Loss -1.7260385751724243
Epoch 65: Training Loss -1.7315303647994995
Epoch 65: Validation Loss -1.7201792853219169
Epoch 66: Training Loss -1.727600203704834
Epoch 66: Validation Loss -1.7265809869009352
Epoch 67: Training Loss -1.7375625991821289
Epoch 67: Validation Loss -1.7390636148906888
Epoch 68: Training Loss -1.7399484470367432
Epoch 68: Validation Loss -1.7392079489571708
Epoch 69: Training Loss -1.7436370780944823
Epoch 69: Validation Loss -1.7483025978481959
Epoch 70: Training Loss -1.7418311393737793
Epoch 70: Validation Loss -1.750108251495967
Epoch 71: Training Loss -1.7478796270370482
Epoch 71: Validation Loss -1.7397696101476277
Epoch 72: Training Loss -1.742604956817627
Epoch 72: Validation Loss -1.7387712437009055
Epoch 73: Training Loss -1.747049208831787
Epoch 73: Validation Loss -1.7527031860654316
Epoch 74: Training Loss -1.7480688634872437
Epoch 74: Validation Loss -1.7578152588435583
Epoch 75: Training Loss -1.745730618095398
Epoch 75: Validation Loss -1.7441958018711634
Epoch 76: Training Loss -1.7467041198730469
Epoch 76: Validation Loss -1.7489485627128964
Epoch 77: Training Loss -1.7462463418960572
Epoch 77: Validation Loss -1.7577362022702656
Epoch 78: Training Loss -1.7474305755615234
Epoch 78: Validation Loss -1.7537324693467882
Epoch 79: Training Loss -1.7485191139221192
Epoch 79: Validation Loss -1.743820485614595
Epoch 80: Training Loss -1.747238618659973
Epoch 80: Validation Loss -1.7525532661922394
Epoch 81: Training Loss -1.7483398128509522
Epoch 81: Validation Loss -1.7488034452710832
Epoch 82: Training Loss -1.7513125555038451
Epoch 82: Validation Loss -1.7457776788681272
Epoch 83: Training Loss -1.7486464330673217
Epoch 83: Validation Loss -1.7638749792462303
Epoch 84: Training Loss -1.7498238018035888
Epoch 84: Validation Loss -1.7488331000010173
Epoch 85: Training Loss -1.7511559852600098
Epoch 85: Validation Loss -1.7525519700277419
Epoch 86: Training Loss -1.7457572793960572
Epoch 86: Validation Loss -1.7363983771157643
Epoch 87: Training Loss -1.7495589500427247
Epoch 87: Validation Loss -1.743029899067349
Epoch 88: Training Loss -1.7498170375823974
Epoch 88: Validation Loss -1.7412229246563382
Epoch 89: Training Loss -1.7202121921539306
Epoch 89: Validation Loss -1.7341730878466652
Epoch 90: Training Loss -1.7529359035491943
Epoch 90: Validation Loss -1.7505321256698123
Epoch 91: Training Loss -1.7562992691040038
Epoch 91: Validation Loss -1.7536014753674705
Epoch 92: Training Loss -1.7562194929122925
Epoch 92: Validation Loss -1.7576057456788563
Epoch 93: Training Loss -1.7542063175201417
Epoch 93: Validation Loss -1.7568436652895003
Epoch 94: Training Loss -1.7572925792694092
Epoch 94: Validation Loss -1.7567567655018397
Epoch 95: Training Loss -1.7611050945281983
Epoch 95: Validation Loss -1.7621340600271074
Epoch 96: Training Loss -1.7610233251571654
Epoch 96: Validation Loss -1.7582717887938968
Epoch 97: Training Loss -1.755621971130371
Epoch 97: Validation Loss -1.7544482132745167
Epoch 98: Training Loss -1.7611895481109618
Epoch 98: Validation Loss -1.7522812551922269
Epoch 99: Training Loss -1.7601272296905517
Epoch 99: Validation Loss -1.7554803821775649
Epoch 100: Training Loss -1.7613703327178956
Epoch 100: Validation Loss -1.7619476734645783
Epoch 101: Training Loss -1.7593786052703857
Epoch 101: Validation Loss -1.7618660529454548
Epoch 102: Training Loss -1.7664841560363769
Epoch 102: Validation Loss -1.7706996807976374
Epoch 103: Training Loss -1.7672209842681885
Epoch 103: Validation Loss -1.7624378753086878
Epoch 104: Training Loss -1.7618111248016357
Epoch 104: Validation Loss -1.7600387845720564
Epoch 105: Training Loss -1.7650110353469848
Epoch 105: Validation Loss -1.7677272179770092
Epoch 106: Training Loss -1.7672691623687744
Epoch 106: Validation Loss -1.7702957316050454
Epoch 107: Training Loss -1.763477949523926
Epoch 107: Validation Loss -1.75689863401746
Epoch 108: Training Loss -1.764906541442871
Epoch 108: Validation Loss -1.7632875839869182
Epoch 109: Training Loss -1.7635038753509522
Epoch 109: Validation Loss -1.765145112597753
Epoch 110: Training Loss -1.7720242010116578
Epoch 110: Validation Loss -1.7706090211868286
Epoch 111: Training Loss -1.7699301330566406
Epoch 111: Validation Loss -1.7631693983834886
Epoch 112: Training Loss -1.7696504262924195
Epoch 112: Validation Loss -1.7715823461139013
Epoch 113: Training Loss -1.77387887134552
Epoch 113: Validation Loss -1.7669055140207683
Epoch 114: Training Loss -1.7698113880157471
Epoch 114: Validation Loss -1.766422952924456
Epoch 115: Training Loss -1.7647310760498047
Epoch 115: Validation Loss -1.7655464940600925
Epoch 116: Training Loss -1.7640543502807617
Epoch 116: Validation Loss -1.7737731044254605
Epoch 117: Training Loss -1.7654656316757202
Epoch 117: Validation Loss -1.770966544983879
Epoch 118: Training Loss -1.769289115524292
Epoch 118: Validation Loss -1.7676617701848347
Epoch 119: Training Loss -1.7699185171127318
Epoch 119: Validation Loss -1.7644857433107164
Epoch 120: Training Loss -1.7720891767501832
Epoch 120: Validation Loss -1.7750540744690668
Epoch 121: Training Loss -1.7693968963623048
Epoch 121: Validation Loss -1.7695973733114818
Epoch 122: Training Loss -1.7728810094833374
Epoch 122: Validation Loss -1.7681661560421897
Epoch 123: Training Loss -1.7712038053512573
Epoch 123: Validation Loss -1.7729649884360177
Epoch 124: Training Loss -1.7726261220932007
Epoch 124: Validation Loss -1.7736135380608695
Epoch 125: Training Loss -1.7761251495361328
Epoch 125: Validation Loss -1.7782292384949943
Epoch 126: Training Loss -1.7741649408340454
Epoch 126: Validation Loss -1.7836210500626337
Epoch 127: Training Loss -1.7780158256530763
Epoch 127: Validation Loss -1.7705300838228255
Epoch 128: Training Loss -1.7728327241897583
Epoch 128: Validation Loss -1.7755643167192974
Epoch 129: Training Loss -1.7755295419692994
Epoch 129: Validation Loss -1.7742027611959548
Epoch 130: Training Loss -1.7751623048782348
Epoch 130: Validation Loss -1.7742067424077836
Epoch 131: Training Loss -1.775542428970337
Epoch 131: Validation Loss -1.7721722542293488
Epoch 132: Training Loss -1.7744264137268067
Epoch 132: Validation Loss -1.7790858064379012
Epoch 133: Training Loss -1.776563702583313
Epoch 133: Validation Loss -1.7732191237192305
Epoch 134: Training Loss -1.775494969367981
Epoch 134: Validation Loss -1.771436027118138
Epoch 135: Training Loss -1.7765250284194947
Epoch 135: Validation Loss -1.766993077974471
Epoch 136: Training Loss -1.7776922227859497
Epoch 136: Validation Loss -1.7785234205306522
Epoch 137: Training Loss -1.7755956407546998
Epoch 137: Validation Loss -1.790069093779912
Epoch 138: Training Loss -1.778984440422058
Epoch 138: Validation Loss -1.775385816891988
Epoch 139: Training Loss -1.7759312774658202
Epoch 139: Validation Loss -1.7742884915972512
Epoch 140: Training Loss -1.7791981378555297
Epoch 140: Validation Loss -1.7782607097474357
Epoch 141: Training Loss -1.7779914804458619
Epoch 141: Validation Loss -1.7738523294055273
Epoch 142: Training Loss -1.7812215127944946
Epoch 142: Validation Loss -1.7776950843750485
Epoch 143: Training Loss -1.7787804334640502
Epoch 143: Validation Loss -1.7801462109126742
Epoch 144: Training Loss -1.7798509864807128
Epoch 144: Validation Loss -1.7794882588916354
Epoch 145: Training Loss -1.7809557281494142
Epoch 145: Validation Loss -1.773816430379474
Epoch 146: Training Loss -1.7807750352859497
Epoch 146: Validation Loss -1.7848960274741763
Epoch 147: Training Loss -1.7790340068817139
Epoch 147: Validation Loss -1.7754099179827978
Epoch 148: Training Loss -1.7824146764755249
Epoch 148: Validation Loss -1.7821132493397547
Epoch 149: Training Loss -1.7796083688735962
Epoch 149: Validation Loss -1.7851573739733015
Epoch 150: Training Loss -1.7816186267852783
Epoch 150: Validation Loss -1.7722458385285877
Epoch 151: Training Loss -1.778939158630371
Epoch 151: Validation Loss -1.779067249525161
Epoch 152: Training Loss -1.780089299583435
Epoch 152: Validation Loss -1.784878655085488
Epoch 153: Training Loss -1.7801889919281007
Epoch 153: Validation Loss -1.7747380847022647
Epoch 154: Training Loss -1.7795230834960938
Epoch 154: Validation Loss -1.783442152871026
Epoch 155: Training Loss -1.7814724813461305
Epoch 155: Validation Loss -1.7781375760123843
Epoch 156: Training Loss -1.7811914941787719
Epoch 156: Validation Loss -1.782052549104842
Epoch 157: Training Loss -1.7835724630355836
Epoch 157: Validation Loss -1.7796773153638084
Epoch 158: Training Loss -1.7817432563781739
Epoch 158: Validation Loss -1.7759222019286383
Epoch 159: Training Loss -1.7833707542419435
Epoch 159: Validation Loss -1.7850324237157429
Epoch 160: Training Loss -1.7839868091583253
Epoch 160: Validation Loss -1.7897078707104637
Epoch 161: Training Loss -1.7827783285140992
Epoch 161: Validation Loss -1.7799911007048592
Epoch 162: Training Loss -1.7834154514312743
Epoch 162: Validation Loss -1.7811047822710067
Epoch 163: Training Loss -1.7854889055252074
Epoch 163: Validation Loss -1.7844332797186715
Epoch 164: Training Loss -1.7824373657226562
Epoch 164: Validation Loss -1.781160523021032
Epoch 165: Training Loss -1.7846800582885742
Epoch 165: Validation Loss -1.7829749716652765
Epoch 166: Training Loss -1.782571152496338
Epoch 166: Validation Loss -1.7862157613512069
Epoch 167: Training Loss -1.782939306640625
Epoch 167: Validation Loss -1.790590314638047
Epoch 168: Training Loss -1.784081411743164
Epoch 168: Validation Loss -1.7906268172793918
Epoch 169: Training Loss -1.7833291381835938
Epoch 169: Validation Loss -1.7765843792567177
Epoch 170: Training Loss -1.783379390335083
Epoch 170: Validation Loss -1.776395737178742
Epoch 171: Training Loss -1.782866469192505
Epoch 171: Validation Loss -1.778147504443214
Epoch 172: Training Loss -1.7850123420715331
Epoch 172: Validation Loss -1.7842206273760115
Epoch 173: Training Loss -1.7850014825820923
Epoch 173: Validation Loss -1.778177751435174
Epoch 174: Training Loss -1.783707290649414
Epoch 174: Validation Loss -1.775661786397298
Epoch 175: Training Loss -1.785315533065796
Epoch 175: Validation Loss -1.785872294789269
Epoch 176: Training Loss -1.7834106702804566
Epoch 176: Validation Loss -1.7914798940931047
Epoch 177: Training Loss -1.7845779689788819
Epoch 177: Validation Loss -1.7838988247371854
Epoch 178: Training Loss -1.7819561584472656
Epoch 178: Validation Loss -1.795472258613223
Epoch 179: Training Loss -1.7846122838974
Epoch 179: Validation Loss -1.7860746932408167
Epoch 180: Training Loss -1.7826997035980225
Epoch 180: Validation Loss -1.7820922041696214
Epoch 181: Training Loss -1.7843227535247803
Epoch 181: Validation Loss -1.7870064444012113
Epoch 182: Training Loss -1.7857228393554687
Epoch 182: Validation Loss -1.7865213420655992
Epoch 183: Training Loss -1.7841573650360107
Epoch 183: Validation Loss -1.787769662009345
Epoch 184: Training Loss -1.7837284019470214
Epoch 184: Validation Loss -1.7852358685599432
Epoch 185: Training Loss -1.783860150527954
Epoch 185: Validation Loss -1.7835291132094369
Epoch 186: Training Loss -1.7853131813049317
Epoch 186: Validation Loss -1.7819928063286676
Epoch 187: Training Loss -1.782710347366333
Epoch 187: Validation Loss -1.7782421906789143
Epoch 188: Training Loss -1.7855948152542114
Epoch 188: Validation Loss -1.7863601786749703
Epoch 189: Training Loss -1.7852328968048097
Epoch 189: Validation Loss -1.784530658570547
Epoch 190: Training Loss -1.7835392486572266
Epoch 190: Validation Loss -1.7857137256198459
Epoch 191: Training Loss -1.7846214643478393
Epoch 191: Validation Loss -1.786554802031744
Epoch 192: Training Loss -1.7833667364120482
Epoch 192: Validation Loss -1.7864645511384993
Epoch 193: Training Loss -1.7834385517120361
Epoch 193: Validation Loss -1.780479221116929
Epoch 194: Training Loss -1.7846903163909913
Epoch 194: Validation Loss -1.7810718086030748
Epoch 195: Training Loss -1.7835568611145018
Epoch 195: Validation Loss -1.7788452439837985
Epoch 196: Training Loss -1.7843764280319214
Epoch 196: Validation Loss -1.7845117970118447
Epoch 197: Training Loss -1.7844291786193847
Epoch 197: Validation Loss -1.7819373721168155
Epoch 198: Training Loss -1.7876433059692383
Epoch 198: Validation Loss -1.7734053286295088
Epoch 199: Training Loss -1.7864786899566651
Epoch 199: Validation Loss -1.7836331639971053
Epoch 200: Training Loss -1.7840816452026367
Epoch 200: Validation Loss -1.7879017856385973
Epoch 201: Training Loss -1.7867337253570557
Epoch 201: Validation Loss -1.7833229189827329
Epoch 202: Training Loss -1.7863065391540527
Epoch 202: Validation Loss -1.789057216947041
Epoch 203: Training Loss -1.7867190238952637
Epoch 203: Validation Loss -1.7843180554253715
Epoch 204: Training Loss -1.7836057849884033
Epoch 204: Validation Loss -1.78164299518343
Epoch 205: Training Loss -1.7854864274978637
Epoch 205: Validation Loss -1.7840342975798107
Epoch 206: Training Loss -1.7851646390914917
Epoch 206: Validation Loss -1.7904597653283014
Epoch 207: Training Loss -1.786535125541687
Epoch 207: Validation Loss -1.787388841311137
Epoch 208: Training Loss -1.7873070878982544
Epoch 208: Validation Loss -1.7898550506622073
Epoch 209: Training Loss -1.783610139274597
Epoch 209: Validation Loss -1.784228105393667
Epoch 210: Training Loss -1.7876160749435426
Epoch 210: Validation Loss -1.7883248064253066
Epoch 211: Training Loss -1.7870154489517212
Epoch 211: Validation Loss -1.784406610897609
Epoch 212: Training Loss -1.7858496046066283
Epoch 212: Validation Loss -1.7764405466261364
Epoch 213: Training Loss -1.784575636291504
Epoch 213: Validation Loss -1.7771326265637837
Epoch 214: Training Loss -1.786118884086609
Epoch 214: Validation Loss -1.7872194638327947
Epoch 215: Training Loss -1.7856421949386596
Epoch 215: Validation Loss -1.782710311904786
Epoch 216: Training Loss -1.7852390846252442
Epoch 216: Validation Loss -1.7846771705718267
Epoch 217: Training Loss -1.7845761791229249
Epoch 217: Validation Loss -1.7765657258412195
Epoch 218: Training Loss -1.786600447845459
Epoch 218: Validation Loss -1.7839141762445843
Epoch 219: Training Loss -1.7881211643218995
Epoch 219: Validation Loss -1.7902354796727498
Epoch 220: Training Loss -1.7848744506835938
Epoch 220: Validation Loss -1.7866538365681965
Epoch 221: Training Loss -1.7849291507720948
Epoch 221: Validation Loss -1.7870476094503251
Epoch 222: Training Loss -1.7858676668167115
Epoch 222: Validation Loss -1.7820200390285916
Epoch 223: Training Loss -1.7815530765533447
Epoch 223: Validation Loss -1.7825391992690072
Epoch 224: Training Loss -1.786379172706604
Epoch 224: Validation Loss -1.783449751990182
Epoch 225: Training Loss -1.7864254671096802
Epoch 225: Validation Loss -1.7931389751888456
Epoch 226: Training Loss -1.7852130502700805
Epoch 226: Validation Loss -1.7866593447942583
Epoch 227: Training Loss -1.7849156526565553
Epoch 227: Validation Loss -1.7904522267598955
Epoch 228: Training Loss -1.7855813133239746
Epoch 228: Validation Loss -1.7820298993398274
Epoch 229: Training Loss -1.786291912651062
Epoch 229: Validation Loss -1.7906878051303683
Epoch 230: Training Loss -1.7830489801406861
Epoch 230: Validation Loss -1.778677917662121
Epoch 231: Training Loss -1.7883699337005616
Epoch 231: Validation Loss -1.792044444689675
Epoch 232: Training Loss -1.7887388038635255
Epoch 232: Validation Loss -1.7918867686438182
Epoch 233: Training Loss -1.7859343336105347
Epoch 233: Validation Loss -1.7838911480373807
Epoch 234: Training Loss -1.7880846784591675
Epoch 234: Validation Loss -1.7807228905814034
Epoch 235: Training Loss -1.7860688499450683
Epoch 235: Validation Loss -1.7959055124767243
Epoch 236: Training Loss -1.7866163583755492
Epoch 236: Validation Loss -1.781465002468654
Epoch 237: Training Loss -1.7893746349334716
Epoch 237: Validation Loss -1.7870209519825284
Epoch 238: Training Loss -1.7839819610595704
Epoch 238: Validation Loss -1.7816445032755535
Epoch 239: Training Loss -1.788210848236084
Epoch 239: Validation Loss -1.784803205066257
Epoch 240: Training Loss -1.787686781311035
Epoch 240: Validation Loss -1.787449588851323
Epoch 241: Training Loss -1.786442250061035
Epoch 241: Validation Loss -1.7901370903802296
Epoch 242: Training Loss -1.7857764156341553
Epoch 242: Validation Loss -1.7889074673728338
Epoch 243: Training Loss -1.7858532598495482
Epoch 243: Validation Loss -1.7849598536415705
Epoch 244: Training Loss -1.7861818614959717
Epoch 244: Validation Loss -1.7896837260988023
Epoch 245: Training Loss -1.7869828887939454
Epoch 245: Validation Loss -1.78645702581557
Epoch 246: Training Loss -1.7863829402923583
Epoch 246: Validation Loss -1.7797607107767983
Epoch 247: Training Loss -1.7875889081954957
Epoch 247: Validation Loss -1.793308725432744
Epoch 248: Training Loss -1.786672206878662
Epoch 248: Validation Loss -1.784061244555882
Epoch 249: Training Loss -1.7871845226287841
Epoch 249: Validation Loss -1.7827688020373147
Best Validation Loss -1.7959055124767243 on Epoch 235
